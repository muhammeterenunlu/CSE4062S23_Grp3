from sklearn.metrics import confusion_matrix, f1_score

def print_confusion_matrix(y_true, y_pred, title="Confusion Matrix"):
    cm = confusion_matrix(y_true, y_pred)
    print(f"\n{title}:")
    print(cm)

def print_decision_tree_info_gain(results_gain, error_rate_train_gain, error_rate_test_gain, y_train_gain, y_train_pred_gain, y_test_gain, y_pred_gain):
    print("Decision Tree using Information Gain")
    print("------------------------------")
    print("Training set results:")
    print("Accuracy:", results_gain['train']['accuracy'])
    print("Recall:", results_gain['train']['recall'])
    print("Precision:", results_gain['train']['precision'])
    print("F1 Score:", results_gain['train']['f1_score'])
    print("Macro F1 Score:", f1_score(y_train_gain, y_train_pred_gain, average='macro'))
    print("Micro F1 Score:", f1_score(y_train_gain, y_train_pred_gain, average='micro'))
    print_confusion_matrix(y_train_gain, y_train_pred_gain, title="Training Set Confusion Matrix")

    print("\nTest set results:")
    print("Accuracy:", results_gain['test']['accuracy'])
    print("Recall:", results_gain['test']['recall'])
    print("Precision:", results_gain['test']['precision'])
    print("F1 Score:", results_gain['test']['f1_score'])
    print("Macro F1 Score:", f1_score(y_test_gain, y_pred_gain, average='macro'))
    print("Micro F1 Score:", f1_score(y_test_gain, y_pred_gain, average='micro'))
    print_confusion_matrix(y_test_gain, y_pred_gain, title="Test Set Confusion Matrix")

    print("\nError Rate (Training):", error_rate_train_gain)
    print("Error Rate (Test):", error_rate_test_gain)

def print_decision_tree_gini_index(results_gini, error_rate_train_gini, error_rate_test_gini, y_train_gini, y_train_pred_gini, y_test_gini, y_pred_gini):
    print("\nDecision Tree using Gini Index")
    print("--------------------------------")
    print("Training set results:")
    print("Accuracy:", results_gini['train']['accuracy'])
    print("Recall:", results_gini['train']['recall'])
    print("Precision:", results_gini['train']['precision'])
    print("F1 Score:", results_gini['train']['f1_score'])
    print("Macro F1 Score:", f1_score(y_train_gini, y_train_pred_gini, average='macro'))
    print("Micro F1 Score:", f1_score(y_train_gini, y_train_pred_gini, average='micro'))
    print_confusion_matrix(y_train_gini, y_train_pred_gini, title="Training Set Confusion Matrix")

    print("\nTest set results:")
    print("Accuracy:", results_gini['test']['accuracy'])
    print("Recall:", results_gini['test']['recall'])
    print("Precision:", results_gini['test']['precision'])
    print("F1 Score:", results_gini['test']['f1_score'])
    print("Macro F1 Score:", f1_score(y_test_gini, y_pred_gini, average='macro'))
    print("Micro F1 Score:", f1_score(y_test_gini, y_pred_gini, average='micro'))
    print_confusion_matrix(y_test_gini, y_pred_gini, title="Test Set Confusion Matrix")

    print("\nError Rate (Training):", error_rate_train_gini)
    print("Error Rate (Test):", error_rate_test_gini)

def print_decision_tree_gradient_boosting(results_gb, error_rate_train_gb, error_rate_test_gb, y_train_gb, y_train_pred_gb, y_test_gb, y_pred_gb):
    print("\nDecision Tree using Gradient Boosting")
    print("-----------------")
    print("Training set results:")
    print("Accuracy:", results_gb['train']['accuracy'])
    print("Recall:", results_gb['train']['recall'])
    print("Precision:", results_gb['train']['precision'])
    print("F1 Score:", results_gb['train']['f1_score'])
    print("Macro F1 Score:", f1_score(y_train_gb, y_train_pred_gb, average='macro'))
    print("Micro F1 Score:", f1_score(y_train_gb, y_train_pred_gb, average='micro'))
    print_confusion_matrix(y_train_gb, y_train_pred_gb, title="Training Set Confusion Matrix")

    print("\nTest set results:")
    print("Accuracy:", results_gb['test']['accuracy'])
    print("Recall:", results_gb['test']['recall'])
    print("Precision:", results_gb['test']['precision'])
    print("F1 Score:", results_gb['test']['f1_score'])
    print("Macro F1 Score:", f1_score(y_test_gb, y_pred_gb, average='macro'))
    print("Micro F1 Score:", f1_score(y_test_gb, y_pred_gb, average='micro'))
    print_confusion_matrix(y_test_gb, y_pred_gb, title="Test Set Confusion Matrix")

    print("\nError Rate (Training):", error_rate_train_gb)
    print("Error Rate (Test):", error_rate_test_gb)

def print_ann_1_hidden_layer_adam(results_ann_1_adam, error_rate_train_ann1_adam, error_rate_test_ann1_adam, y_train_ann1_adam, y_train_pred_ann1_adam, y_test_ann1_adam, y_pred_ann1_adam):
    print("\nANN with 1 Hidden Layer (ADAM Optimizer)")
    print("-------------------------")
    print("Training set results:")
    print("Accuracy:", results_ann_1_adam['train']['accuracy'])
    print("Recall:", results_ann_1_adam['train']['recall'])
    print("Precision:", results_ann_1_adam['train']['precision'])
    print("F1 Score:", results_ann_1_adam['train']['f1_score'])
    print("Macro F1 Score:", f1_score(y_train_ann1_adam, y_train_pred_ann1_adam, average='macro'))
    print("Micro F1 Score:", f1_score(y_train_ann1_adam, y_train_pred_ann1_adam, average='micro'))

    print("\nTest set results:")
    print("Accuracy:", results_ann_1_adam['test']['accuracy'])
    print("Recall:", results_ann_1_adam['test']['recall'])
    print("Precision:", results_ann_1_adam['test']['precision'])
    print("F1 Score:", results_ann_1_adam['test']['f1_score'])
    print("Macro F1 Score:", f1_score(y_test_ann1_adam, y_pred_ann1_adam, average='macro'))
    print("Micro F1 Score:", f1_score(y_test_ann1_adam, y_pred_ann1_adam, average='micro'))

    print("\nError Rate (Training):", error_rate_train_ann1_adam)
    print("Error Rate (Test):", error_rate_test_ann1_adam)

def print_ann_1_hidden_layer_sgd(results_ann_1_sgd, error_rate_train_ann1_sgd, error_rate_test_ann1_sgd, y_train_ann1_sgd, y_train_pred_ann1_sgd, y_test_ann1_sgd, y_pred_ann1_sgd):
    print("\nANN with 1 Hidden Layer (SGD Optimizer)")
    print("-----------------------------------------")
    print("Training set results:")
    print("Accuracy:", results_ann_1_sgd['train']['accuracy'])
    print("Recall:", results_ann_1_sgd['train']['recall'])
    print("Precision:", results_ann_1_sgd['train']['precision'])
    print("F1 Score:", results_ann_1_sgd['train']['f1_score'])
    print("Macro F1 Score:", f1_score(y_train_ann1_sgd, y_train_pred_ann1_sgd, average='macro'))
    print("Micro F1 Score:", f1_score(y_train_ann1_sgd, y_train_pred_ann1_sgd, average='micro'))

    print("\nTest set results:")
    print("Accuracy:", results_ann_1_sgd['test']['accuracy'])
    print("Recall:", results_ann_1_sgd['test']['recall'])
    print("Precision:", results_ann_1_sgd['test']['precision'])
    print("F1 Score:", results_ann_1_sgd['test']['f1_score'])
    print("Macro F1 Score:", f1_score(y_test_ann1_sgd, y_pred_ann1_sgd, average='macro'))
    print("Micro F1 Score:", f1_score(y_test_ann1_sgd, y_pred_ann1_sgd, average='micro'))

    print("\nError Rate (Training):", error_rate_train_ann1_sgd)
    print("Error Rate (Test):", error_rate_test_ann1_sgd)

def print_ann_1_hidden_layer_rmsprop(results_ann_1_rmsprop, error_rate_train_ann1_rmsprop, error_rate_test_ann1_rmsprop, y_train_ann1_rmsprop, y_train_pred_ann1_rmsprop, y_test_ann1_rmsprop, y_pred_ann1_rmsprop):
    print("\nANN with 1 Hidden Layer (RMSprop Optimizer)")
    print("---------------------------------------------")
    print("Training set results:")
    print("Accuracy:", results_ann_1_rmsprop['train']['accuracy'])
    print("Recall:", results_ann_1_rmsprop['train']['recall'])
    print("Precision:", results_ann_1_rmsprop['train']['precision'])
    print("F1 Score:", results_ann_1_rmsprop['train']['f1_score'])
    print("Macro F1 Score:", f1_score(y_train_ann1_rmsprop, y_train_pred_ann1_rmsprop, average='macro'))
    print("Micro F1 Score:", f1_score(y_train_ann1_rmsprop, y_train_pred_ann1_rmsprop, average='micro'))

    print("\nTest set results:")
    print("Accuracy:", results_ann_1_rmsprop['test']['accuracy'])
    print("Recall:", results_ann_1_rmsprop['test']['recall'])
    print("Precision:", results_ann_1_rmsprop['test']['precision'])
    print("F1 Score:", results_ann_1_rmsprop['test']['f1_score'])
    print("Macro F1 Score:", f1_score(y_test_ann1_rmsprop, y_pred_ann1_rmsprop, average='macro'))
    print("Micro F1 Score:", f1_score(y_test_ann1_rmsprop, y_pred_ann1_rmsprop, average='micro'))

    print("\nError Rate (Training):", error_rate_train_ann1_rmsprop)
    print("Error Rate (Test):", error_rate_test_ann1_rmsprop)

def print_linear_svm(results_svm, error_rate_train_svm, error_rate_test_svm, y_train_svm, y_train_pred_svm, y_test_svm, y_pred_svm):
    print("Linear SVM")
    print("------------------------------")
    print("Training set results:")
    print("Accuracy:", results_svm['train']['accuracy'])
    print("Recall:", results_svm['train']['recall'])
    print("Precision:", results_svm['train']['precision'])
    print("F1 Score:", results_svm['train']['f1_score'])
    print("Macro F1 Score:", f1_score(y_train_svm, y_train_pred_svm, average='macro'))
    print("Micro F1 Score:", f1_score(y_train_svm, y_train_pred_svm, average='micro'))
    print_confusion_matrix(y_train_svm, y_train_pred_svm, title="Training Set Confusion Matrix")

    print("\nTest set results:")
    print("Accuracy:", results_svm['test']['accuracy'])
    print("Recall:", results_svm['test']['recall'])
    print("Precision:", results_svm['test']['precision'])
    print("F1 Score:", results_svm['test']['f1_score'])
    print("Macro F1 Score:", f1_score(y_test_svm, y_pred_svm, average='macro'))
    print("Micro F1 Score:", f1_score(y_test_svm, y_pred_svm, average='micro'))
    print_confusion_matrix(y_test_svm, y_pred_svm, title="Test Set Confusion Matrix")

    print("\nError Rate (Training):", error_rate_train_svm)
    print("Error Rate (Test):", error_rate_test_svm)

def print_bernoulli_naive_bayes(results_nb_bernoulli, error_rate_train_nb_bernoulli, error_rate_test_nb_bernoulli, y_train_nb_bernoulli, y_train_pred_nb_bernoulli, y_test_nb_bernoulli, y_pred_nb_bernoulli):
    print("Bernoulli Naive Bayes")
    print("---------------------")
    print("Training set results:")
    print("Accuracy:", results_nb_bernoulli['train']['accuracy'])
    print("Recall:", results_nb_bernoulli['train']['recall'])
    print("Precision:", results_nb_bernoulli['train']['precision'])
    print("F1 Score:", results_nb_bernoulli['train']['f1_score'])
    print("Macro F1 Score:", f1_score(y_train_nb_bernoulli, y_train_pred_nb_bernoulli, average='macro'))
    print("Micro F1 Score:", f1_score(y_train_nb_bernoulli, y_train_pred_nb_bernoulli, average='micro'))
    print_confusion_matrix(y_train_nb_bernoulli, y_train_pred_nb_bernoulli, title="Training Set Confusion Matrix")

    print("\nTest set results:")
    print("Accuracy:", results_nb_bernoulli['test']['accuracy'])
    print("Recall:", results_nb_bernoulli['test']['recall'])
    print("Precision:", results_nb_bernoulli['test']['precision'])
    print("F1 Score:", results_nb_bernoulli['test']['f1_score'])
    print("Macro F1 Score:", f1_score(y_test_nb_bernoulli, y_pred_nb_bernoulli, average='macro'))
    print("Micro F1 Score:", f1_score(y_test_nb_bernoulli, y_pred_nb_bernoulli, average='micro'))
    print_confusion_matrix(y_test_nb_bernoulli, y_pred_nb_bernoulli, title="Test Set Confusion Matrix")

    print("\nError Rate (Training):", error_rate_train_nb_bernoulli)
    print("Error Rate (Test):", error_rate_test_nb_bernoulli)

def print_multinomial_naive_bayes(results_nb_multinomial, error_rate_train_nb_multinomial, error_rate_test_nb_multinomial, y_train_nb_multinomial, y_train_pred_nb_multinomial, y_test_nb_multinomial, y_pred_nb_multinomial):
    print("Multinomial Naive Bayes")
    print("-----------------------")
    print("Training set results:")
    print("Accuracy:", results_nb_multinomial['train']['accuracy'])
    print("Recall:", results_nb_multinomial['train']['recall'])
    print("Precision:", results_nb_multinomial['train']['precision'])
    print("F1 Score:", results_nb_multinomial['train']['f1_score'])
    print("Macro F1 Score:", f1_score(y_train_nb_multinomial, y_train_pred_nb_multinomial, average='macro'))
    print("Micro F1 Score:", f1_score(y_train_nb_multinomial, y_train_pred_nb_multinomial, average='micro'))
    print_confusion_matrix(y_train_nb_multinomial, y_train_pred_nb_multinomial, title="Training Set Confusion Matrix")

    print("\nTest set results:")
    print("Accuracy:", results_nb_multinomial['test']['accuracy'])
    print("Recall:", results_nb_multinomial['test']['recall'])
    print("Precision:", results_nb_multinomial['test']['precision'])
    print("F1 Score:", results_nb_multinomial['test']['f1_score'])
    print("Macro F1 Score:", f1_score(y_test_nb_multinomial, y_pred_nb_multinomial, average='macro'))
    print("Micro F1 Score:", f1_score(y_test_nb_multinomial, y_pred_nb_multinomial, average='micro'))
    print_confusion_matrix(y_test_nb_multinomial, y_pred_nb_multinomial, title="Test Set Confusion Matrix")

    print("\nError Rate (Training):", error_rate_train_nb_multinomial)
    print("Error Rate (Test):", error_rate_test_nb_multinomial)

def print_gaussian_naive_bayes(results_nb_gaussian, error_rate_train_nb_gaussian, error_rate_test_nb_gaussian, y_train_nb_gaussian, y_train_pred_nb_gaussian, y_test_nb_gaussian, y_pred_nb_gaussian):
    print("Gaussian Naive Bayes")
    print("--------------------")
    print("Training set results:")
    print("Accuracy:", results_nb_gaussian['train']['accuracy'])
    print("Recall:", results_nb_gaussian['train']['recall'])
    print("Precision:", results_nb_gaussian['train']['precision'])
    print("F1 Score:", results_nb_gaussian['train']['f1_score'])
    print("Macro F1 Score:", f1_score(y_train_nb_gaussian, y_train_pred_nb_gaussian, average='macro'))
    print("Micro F1 Score:", f1_score(y_train_nb_gaussian, y_train_pred_nb_gaussian, average='micro'))
    print_confusion_matrix(y_train_nb_gaussian, y_train_pred_nb_gaussian, title="Training Set Confusion Matrix")

    print("\nTest set results:")
    print("Accuracy:", results_nb_gaussian['test']['accuracy'])
    print("Recall:", results_nb_gaussian['test']['recall'])
    print("Precision:", results_nb_gaussian['test']['precision'])
    print("F1 Score:", results_nb_gaussian['test']['f1_score'])
    print("Macro F1 Score:", f1_score(y_test_nb_gaussian, y_pred_nb_gaussian, average='macro'))
    print("Micro F1 Score:", f1_score(y_test_nb_gaussian, y_pred_nb_gaussian, average='micro'))
    print_confusion_matrix(y_test_nb_gaussian, y_pred_nb_gaussian, title="Test Set Confusion Matrix")

    print("\nError Rate (Training):", error_rate_train_nb_gaussian)
    print("Error Rate (Test):", error_rate_test_nb_gaussian)